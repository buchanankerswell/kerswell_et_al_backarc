---
journal: Geochemistry, Geophysics, Geosystems
classoption: draft,linenumbers
title: A comparison of global heat flow interpolation techniques
authors:
- name: Buchanan C. Kerswell
  affil: 1
- name: Matthew J. Kohn
  affil: 1
affiliations:
- number: 1
  name: Department of Geosicences, Boise State University, Boise, ID 83725
corresponding_author:
- name: Buchanan C. Kerswell
  email: buchanankerswell@u.boisestate.edu
keypoints:
  -
  -
  -
abstract:
plain_language_summary:
output:
  rticles::agu_article:
  # redoc::redoc:
    pandoc_args: ['-Fpandoc-crossref', '--csl=g3.csl']
    citation_package: default
bibliography: ref.bib
figPrefix:
- Figure
- Figures
eqnPrefix:
- Equation
- Equations
tblPrefix:
- Table
- Tables
header-includes:
- \usepackage{soulutf8}
- \usepackage{setspace}
- \usepackage{caption}
- \captionsetup[figure]{font={stretch=0.6, footnotesize}}
- \usepackage{hyperref}
- \usepackage{amsmath}
- \usepackage{amsfonts}
- \usepackage{float}
---

```{r echo=FALSE}
# Some recommended settings
knitr::opts_chunk$set(
  echo = FALSE,
  message = F,
  fig.pos = 'h',
  out.extra = "",
  fig.align = 'center',
  out.width = '100%'
)

# Load data and libraries
source('../functions.R')
load('../data/hf.Rdata')

# Load additional libraries for rmd
suppressMessages({
  library(knitr)
  library(kableExtra)
  library(scales)
  library(pander)
})

# Draw bounding boxes and crop heat flow for each segment
shp.hf.crop <- purrr::map_df(shp.sa.segs.robin.pacific$segment,
						~ {box <- shp.sa.segs.robin.pacific.buffer %>%
										filter(segment == .x) %>%
										st_bbox() %>%
										bbox_widen(proj4.robin.pacific,
															 borders = c('top' = 0.1,
																					 'bottom' = 0.1,
																					 'left' = 0.1,
																					 'right' = 0.1))
							shp.hf %>%
											st_crop(box) %>%
											mutate('segment' = .x, .before = country)})

# Read krige results
load_data <- function(x){
#loads an RData file, and returns it
    load(x)
		l <- ls()[!ls() %in% x] %>%
			purrr::map(~get(.x))
		l[1:length(l)-1]
}

# Define paths and names
fname <- c('Alaska Aleutians', 'Andes', 'Central America', 'Kamchatka Marianas',
					 'Kyushu Ryukyu', 'Lesser Antilles', 'New Britain Solomon', 'N. Philippines',
					 'Sumatra Banda Sea', 'Scotia', 'S. Philippines', 'Tonga New Zealand', 'Vanuatu')
fpath <- list.files('../process/', pattern = '.RData', full.names = T)

# Read data
d <- purrr::map2(fpath, fname, ~{
	d <- .x %>% load_data()
	d %>% set_names(c('k', 'shp.hf.pred'))
	}) %>% set_names(fname)
```

# Introduction

Heat escaping the solid Earth's surface indicates a dynamically cooling planet. Surface heat flow databases [@pollack1993; @hasterok2008; @lucazeau2019] provide a way to understand geodynamics by relating the amount of heat escaping Earth's surface to heat-transferring and heat-generating subsurface processes such as diffusion, hydrothermal circulation, radioactive decay, fault motion, subduction dynamics, and mantle convection [@fourier1827; @parsons1977; @pollack1977; @stein1992; @hasterok2013; @stein1994; @rudnick1998; @furlong2013; @gao2014; @furukawa1993; @wada2009; @kerswell2020; @currie2004; @currie2006]. Surface heat flow observations continue to motivate research, evident by more than 1,393 publications compiled in the most recent heat flow dataset, although the rate of publications using surface heat flow has declined since the mid 1980's [@jennings2021].

Many research questions, such as calculating the global surface heat flux from continents and oceans, require interpolating discrete heat flow observations onto a continuous approximation of Earth's surface. Previous attempts at interpolation use one or more geographic, geologic, geochronologic, or geophysical proxies to predict heat flow at unknown locations by association with similar observation sites [e.g., bathymetry or elevation, proximity to active or ancient orogens, seafloor age, upper mantle shear wave velocities, @chapman1975; @lee1965; @sclater1970; @davies2013; @shapiro2004; @goutorbe2011; @lucazeau2019]. These methods are called *similarity methods* (@fig:lucahf). The success of such interpolations are typically evaluated statistically by the misfit between the predicted and observed heat flow. However, even statistically-successful heat flow interpolations are difficult to interpret and show unexpected anomalies [@lucazeau2019]. The fidelity and usefulness of interpolations depend on the question being asked and the choice of methodology.

```{r lucahf, fig.cap='The NGHF dataset. (a) The complete dataset (n = 69729), and (b) interpolation by similarity method from Lucazeau (2019).'}
knitr::include_graphics('../figs/base/hf_luca.png')
```

Predicting surface heat flow by association with physical proxies is arguably the most reasonable approach to interpolation for global investigations. Our understanding of geodynamics and near-surface heat flow perturbations implies that the variance in surface heat flow is not uniformly stochastic, but rather, in large part, determined by the physical conditions and processes operating locally [e.g., @goutorbe2011]. For example, younger oceanic plates should have higher surface heat flow than older plates [@stein1992], subducting oceanic plates will lower surface heat flow near trenches [@furukawa1993], and hydrothermal circulation of seawater can modify heat flow in oceanic crust [@hasterok2011]. Interpolation by association with physical proxies makes reasoned predictions of heat flow based on many independently-tested geodynamic models. However, similarity methods are strongly biased towards such models and risk making determinations where, in fact, surprising results and idiosyncrasies may be found.

In contrast, there exists some degree of stationarity, spatial dependence, or continuity, in the distribution of surface heat flow. A pair of surface heat flow observations taken one meter apart will be strongly correlated. The correlation between pairs of observations will likely decrease with increasing distance between the pairs [@goovaerts1997]. The spatial (dis)continuity of surface heat flow represents the areal extent of geodynamic processes and their interactions. For example, consistent patterns of heat flow near volcanic arcs are interpreted to reflect common backarc lithospheric thermal structures and slab-mantle mechanical coupling depths in subduction zones [@furukawa1993; @wada2009; @kerswell2020].

In theory, one may predict surface heat flow at unknown locations by considering many nearby observations [i.e. Kriging, @krige1951]. However, Kriging is disadvantageous for global interpolations of surface heat flow because it assumes that the underlying distribution of heat flow is stationary (constant in space), which effectively ignores geodynamic complexity. One can overcome this problem by relaxing assumptions of stationarity, or applying Markov-Bayes techniques to include proxies as priors [@bardossy1997]. Instead, we leverage the properties of stationarity as a tool for comparison with *similarity* methods of interpolation [@goutorbe2011; @lucazeau2019]. So the questions are: 1) What are the differences between Kriging and similarity methods? 2) What are the implications of the differences according to the implicit assumptions in both methods?

We attempt to answer these questions by using ordinary Kriging to interpolate the New Global Heat Flow (NGHF) dataset of @lucazeau2019. Our method is optimized using a genetic algorithm to minimize an objective function which considers both the misfit on the variogram models and interpolation results [after @li2018]. We then compare our interpolation results to those of @lucazeau2019 and consider the implications of Kriging vs. similarity methods of interpolation. We restrict our comparison to areas near subduction zone segments defined by @syracuse2006 for two reasons: 1) to provide maps and statistics useful to subduction zone research, and 2) to emphasize differences and idiosyncrasies in both interpolation approaches in a complex tectonic and thermal setting.

# Methods

## The NGHF Dataset

The NGHF dataset was downloaded from the supplementary material of @lucazeau2019. It contains 69729 data points, their locations in latitude/longitude, and metadata---including a data quality rank (Code 6) from A to D (with Code 6 = Z = undetermined). The reader is referred to @lucazeau2019 for details on compilation, references, and historical perspective on the NGHF and previous compilations. We use NGFH because it is the most recent dataset available, has been carefully compiled, and is open-access.

Like @lucazeau2019, we exclude 4790 poor quality observations (Code 6 = D) from our analysis. We further remove 350 data points without heat flow observations and two without geographic information. Multiple observations at the same location are parsed to avoid singular covariance matrices during Kriging:

$$\begin{aligned}
	f(X_i^q, Y_i^q) &= \\
	X_i^q > Y_i^q &\rightarrow z_i = x_i \\
	X_i^q < Y_i^q &\rightarrow z_i = y_i \\
	X_i^q = Y_i^q &\rightarrow z_i = RAND(x_i, y_i)
	\end{aligned}$$ {#eq:parse}

where $X_i^q$ and $Y_i^q$ represent the quality of each duplicate observation pair at location $i$, $RAND$ is a random function that selects either the observation $x_i$ or $y_i$, and $z_i$ stores the observation selected by $f(X_i^q, Y_i^q)$. The final dataset used for Kriging has $n=$ 55274 observations after parsing $n=$ 32430 duplicate observation.

## Kriging

Kriging is a three-step process that involves first estimating an experimental variogram, $\hat{\gamma}(h)$, fitting the experimental variogram with one of many variogram models, $\gamma(h)$, and finally using the modelled variogram to predict random variables at unknown locations [@krige1951; @cressie2015]. We use the general-purpose functions defined in the "R" package `gstat` [@graler2016; @pebesma2004] to perform all three steps. We begin by estimating an experimental variogram as defined by @bardossy1997:

$$\hat{\gamma}(h) = \frac{1}{2N(h)}\sum_{N(h)}^{}(Z(u_i) - Z(u_j))^2$$ {#eq:variogram}

where $N(h)$ is the number of pairs of points, $Z(u_i)$ and $Z(u_j)$, separated by a lag distance, $h = |u_i - u_j|$. We evaluate $\hat{\gamma}(h)$ at fifteen lag distances by binning the irregular spaced data with a bin width, $\delta$, equal to one-third of the maximum lag distance divided by the number of lags used to evaluate the variogram, $\delta = \max (N(h))/(3\cdot 15)$. Then $N(h) \leftarrow N(h, \delta h) = \{i,j:|u_i - u_j| \in [h - \delta h, h + \delta h)\}$. In simple terms, @eq:variogram represents the similarity, or dissimilarity, between pairs of observations in space. @eq:variogram is derived from  the theory of *regionalized variables* [@matheron1963; @matheron2019], which formally defines a probabilistic framework for spatial interpolation of natural phenomena. It is important for the reader to understand the fundamental assumptions implicit in @eq:variogram in order to understand the comparison of interpolation techniques discussed later. The basic assumptions used in our Kriging method are:

- $\hat{\gamma}(h)$ is directionally invariant (isotropic)

- $\hat{\gamma}(h)$ is evaluated in two-dimensions and neglects elevation, $Z(u) \in \mathbb{R}^2$

- The first and second moments of $Z(u)$ have the following conditions over the domain $D$:

	$$\begin{aligned}
		&E[Z(u)] = mean = constant, &\forall u \in D \\
		&E[(Z(u + h) - mean)(Z(u) - mean)] = C(h), &\forall |u, u + h| \in D
		\end{aligned}$$ {#eq:assumptions}

The last assumption (@eq:assumptions) is called "second-order stationarity" and is commonly used in practice. It assumes the underlying probability distribution of the random variable, $Z(u)$, does not change in space and the covariance, $C(h)$, only depends on the distance, $h$, between two random variables. These assumptions are expected to be valid in cases where the underlying natural process is stochastic, spatially continuous, and has the property of additivity such that $\frac{1}{n}\sum_{i=1}^n Z(u_i)$ has the same meaning as $Z(u)$ [@bardossy1997].

The following are two illustrative cases where @eq:assumptions is likely valid:

1. The thickness of a sedimentary unit with a homogeneous concentration of radioactive elements can be approximated by $q_s = q_b + \int A \,dz$, where $q_b$ is a constant heat flux entering the bottom of the layer and $A$ is the heat production within the layer with thickness $z$ [@furlong2013]. If we have two samples, $Z(u_1) = 31~mW/m^2$ and $Z(u_2) = 30.5~mW/m^2$, their corresponding thicknesses would be $Z'(u_1) = 1000~m$ and $Z'(u_2) = 500~m$ for $A = 0.001~mW/m^3$ and $q_b = 30~mW/m^2$. The variable, $Z(u)$, in this case is additive because the arithmetic mean of the samples is a good approximation of the average sedimentary layer thickness, $(Z(u_1) + Z(u_2)) / 2 = 750~m$.

2. The age of young oceanic lithosphere can be approximated by $q_s(t) = kT_b(\pi\kappa t)^{-1/2}$, where $q_s(t)$ is the surface heat flow of a plate with age, $t$, $T_b$ is the temperature at the base of the plate, $k$ is thermal conductivity, and $\kappa = k/\rho C_p$ is thermal diffusivity [@stein1992]. For $k = 3.138~W/mK$, $\rho = 3330~kg/m^3$, $C_p = 1171~J/kgK$, $T_b = 1350^{\circ}C$, two samples, $Z(u_1) = 180~mW/m^2$ and $Z(u_2) = 190~mW/m^2$, would correspond to plates with ages of $Z'(u_1) = 10~Ma$, and $Z'(u_2) = 9~Ma$, respectively. Since $Z(u_1) + Z(u_2) / 2 = 185~mW/m^2$ and $Z'(185~mW/m^2) = 9.5~Ma = Z'(u_1) + Z'(u_2) / 2$, the variable $Z(u)$ in this case is also additive.

In contrast, @eq:assumptions is likely invalid in regions that transition among two or more tectonic regimes. For example, the expected heat flow $E[Z(u)] = mean$ will change when moving from a spreading center to a subduction zone. $E[Z(u)] = mean \neq constant$ over the region of interest. Proceeding with @eq:assumptions in this case has the effect of masking the geodynamic complexity. In other words, the spatial dependence is considered in the Kriging method in this case, but the geodynamic structure is *invisible*. We will see that this has important implications when comparing our Kriging method to @lucazeau2019's interpolation method, which is exactly opposite of this formalism---it only considers the similarities among physical proxies and not spatial dependence.

The second step is to fit the experimental variogram with a variogram model, $\gamma(h)$. In this study we fit three popular variogram models to the experimental variogram. We use models with sills, which implies the spatial dependence between pairs of points has a finite range. The spherical, exponential, and Guassian variogram models are defined as [@chiles2009; @cressie2015]:

$$
\begin{aligned}
	sph &\leftarrow \gamma(h) =
		\begin{cases}
			n + s \left(\frac{3h}{2a} - \frac{1}{2}\left(\frac{h}{a}\right)^3\right), & \quad\text{if } 0 \leq h \leq a \\
			n + s, & \quad\text{if } h > a
		\end{cases} \\
	exp &\leftarrow \gamma(h) = n + s \left(1 - exp\left(\frac{-h}{a}\right)\right), ~\quad\text{if } h \geq 0 \\
	Gau &\leftarrow \gamma(h) = n + s \left(1 - exp\left(\frac{-h^2}{a^2}\right)\right), \quad\text{if } h \geq 0
\end{aligned}
$$ {#eq:varmodels}

where $n$ is the nugget, $s$ is the sill, and $a$ is the effective range. For spherical, exponential, and Gaussian models, the effective range is related to the range, $r$, by $a = r$, $a = r/3$, and $a=1/r\sqrt{3}$, respectively [@graler2016; @pebesma2004]. The function `fit.variogram` in `gstat` allows one to try many variogram models and the best will be selected by the minimum misfit by weighted least square [WLS, @pebesma2004].

We use ordinary Kriging for our interpolation step, which predicts the value of a random function, $\hat{Z}(u)$ at unknown locations as a linear combination of all known locations in the domain, $D$ [@bardossy1997]:

$$ \hat{Z}(u) = \sum_{i=1}^n \lambda_i Z(u_i), \quad \forall u \in D $$ {#eq:linestimate}

The conditions in @eq:assumptions set up a constrained minimization problem since one has:

$$ E[Z(u)] = mean, \quad \forall u \in D $$ {#eq:firstmoment}

The linear estimator must obey

$$ E[\hat{Z}(u)] = \sum_{i=1}^n \lambda_i E[Z(u_i)] = mean $$ {#eq:explinestimate}

so the weights must be:

$$ \sum_{i=1}^n \lambda_i = 1 $$ {#eq:unbiased}

This is the first constraint, also known as the unbiased condition, which states that the sum of the weights must equal one. However, there is an infinite set of real numbers one could use for the weights, $\lambda_i$. Our goal is to find the set of weights in @eq:linestimate that minimizes the estimation variance. This can be solved with the covariance function, $C(h)$ from @eq:assumptions:

$$
\begin{aligned}
	\sigma^2(u) = Var[Z(u) - \hat{Z}(u)] = E\left[(Z(u) - \sum_{i=1}^n \lambda_i Z(u_i))^2\right] &= \\
	E\left[Z(u)^2 + \sum_{j=1}^n \sum_{i=1}^n \lambda_j \lambda_i Z(u_j)Z(u_i) - 2 \sum_{i=1}^n \lambda_i Z(u_i)Z(u)\right] &= \\
	C(0) + \sum_{j=1}^n \sum_{i=1}^n \lambda_j \lambda_i C(u_i - u_j) - 2 \sum_{i=1}^n \lambda_i C(u_i - u)
\end{aligned}
$$ {#eq:minvar}

Solving for the weights in @eq:linestimate with respect to the unbiased condition (@eq:unbiased) and minimum estimate variance (@eq:minvar), yields the best linear unbiased estimator [BLUE, @bardossy1997]. In our case, this is done by the function `krige` in `gstat`.

## Kriging Optimization

Achieving a useful Kriging results depends on one's choice of many Kriging parameters ($\Theta$). In this study, we investigate a set of parameters, $\Theta$:

$$ \Theta = \{m, s, a, n, S\} $$ {#eq:params}

where $m$ is the model type (sph, exp, or Gau), $s$ is the sill, $a$ is the effective range, $n$ is the nugget, and $S$ is the maximum distance for local Kriging. Only points within $S$ from the prediction location are used for Kriging. Our goal is to find $\Theta$ such that our interpolation, $f(x_i; \Theta)$, gives the most useful outcome---defined by minimizing a cost function, $C(\Theta)$, that represents the error between the set of real observations, $Z(u_i)$ and predictions, $\hat{Z}(u)$.

We define a cost function that simultaneously considers the misfit between the experimental and modelled variogram and between the Kriging predictions and observed heat flow [after @li2018]:

$$ C(\Theta) = (1-w)C_F(\Theta) + wC_I(\Theta) $$ {#eq:cost}

where $C_F(\Theta)$ is the root mean square error (RMSE) of the modelled variogram fit calculated by WLS, and $C_I(\Theta)$ is the RMSE of the Kriging result calculated by cross-validation. The weight, $w$, is set to 0.5 in our study, which balances the effects of $C_F(\Theta)$ and $C_I(\Theta)$ on the cost function. The final expression to minimize becomes:

$$
\begin{aligned}
	\min(C(\Theta)) =
	\frac{1-w}{\sigma_E}&\sqrt{\frac{1}{N}\sum_{k=1}^{N}w(h_k)[\hat{\gamma}(h_k)-\gamma(h_k;\Theta)]^2} \quad + \\
	\frac{w}{\sigma_S}&\sqrt{\frac{1}{M}\sum_{i=1}^{M}[Z(u_i)-\hat{Z}(u_i;\Theta)]^2}
\end{aligned}
$$ {#eq:costexp}

where N is the number of pairs of points used to calculate the experimental variogram, $\hat{\gamma}(h_k)$, $\sigma_E$ is the standard deviation of the experimental variogram, $\hat{\gamma}(h)$, $w(h_k)$ is the weight in WLS and defines the importance of the $kth$ lag in the error estimate. We use $w(h_k) = N_k/h_k^2$. $Z(u_i)$ and $\hat{Z}(u_i; \Theta)$ are the measured and predicted values, respectively, $\sigma_s$ is the standard deviation of the predicted values, $\hat{Z}(u_i)$, and M is the number of measurements in $Z(u_i)$. For $C_I(\Theta)$ we use ten-fold cross-validation, which splits the dataset, $|Z(u_i), ~\forall u_i \in D|$ into ten equal intervals and tests one interval against the remaining nine. This process is then repeated over all intervals so that the whole dataset has been cross-validated.

Minimization of $C(\Theta)$ is achieved by a genetic algorithm that simulates biologic natural selection by differential success [@goldberg1989]. Our procedure is as follows:

1. Initiate fifty *chromosomes*, $\xi$, with random starting parameters defined within the search domain (@tbl:search)

2. Evaluate the fitness of each individual chromosome as $-C(\Theta)$ for the entire population

3. Allow the population to exchange genetic information by sequentially performing genetic operations:

	 a. Selection: the top 5% fittest chromosomes survive each generation

	 b. Crossover: pairs of chromosomes have an 80% chance of exchanging genetic information

	 c. Mutation: there is a 10% chance for random genetic mutations

4. Evaluate the fitness of the new population

5. If the termination criterion is met, do step (6), otherwise continue to evolve by repeating steps (3) and (4)

6. Decode the best chromosome and build the optimal variogram

We use the general-purpose functions in the "R" package `GA` [@scrucca2013; @scrucca2016] to perform each step in the above procedure.

|      Parameter      |                 Search Domain               |          Units          |
|---------------------|:-------------------------------------------:|------------------------:|
|      Model (m)      |     [Spherical, Exponential, Guassian]      |            NA           |
|       Sill (s)      |              [1, $5\times 10^3$]            | $\left(mW/m^2\right)^2$ |
| Effective Range (a) |              [1, $1\times 10^6$]            |          meters         |
|      Nugget (n)     |              [1, $1\times 10^3$]            |          meters         |
|   Local Search (S)  |              [1, $1\times 10^6$]            |          meters         |

: Parameters and ranges used in the optimization algorithm {#tbl:search}

## Map Projection and Interpolation Grid

We interpolate onto the same 0.5$^{\circ}$C x 0.5$^{\circ}$C grid as @lucazeau2019 so a direct difference could be calculated between our interpolation methods and @lucazeau2019's. The NGHF and grid with predicted heat flow from @lucazeau2019 were transformed into a Pacific-centered Robinson coordinate reference system (CRS) defined using the `proj` string [@proj2021]:

~~~
+proj=robin +lon_0=-155 +lon_wrap=-155 +x_0=0 +y_0=0
+ellps=WGS84 +datum=WGS84 +units=m +no_defs
~~~

All geographic operations, including Kriging, are performed in the above CRS using the general-purpose functions in the "R" package `sf` [@pebesma2018]. We define the Kriging domain near individual arc segments in two steps: 1) 1000 $km$ buffers are drawn around the arc segments as defined by @syracuse2006. 2) The bounding box of the 1000 $km$ buffer is expanded by 10% on all sides (@fig:segments). We provide the complete NGHF dataset [@lucazeau2019], filtered and parsed NGHF dataset, heat flow interpolations [from @lucazeau2019, and this study], and our code as supplementary information to support FAIR data policy [@wilkinson2016]. These items can also be retrieved from the official repository at [https://doi.org/10.17605/OSF.IO/CA6ZU](https://doi.org/10.17605/OSF.IO/CA6ZU).

```{r segments, fig.cap="Subduction zone segments and interpolation domain. (a) Heat flow is interpolated around thirteen subduction zone segments by (b) drawing a 1000km buffer (lightest blue) around each segment and expanding the buffer's bounding box (medium blue) by 10\\% on all sides (darkest blue). (c) The NGHF dataset is cropped within the largest rectangle. Data from Syracuse \\& Abers (2006) and Lucazeau (2019)."}
knitr::include_graphics('../figs/base/segs_comp.png')
```

\clearpage

# Results
## Heat Flow Near Subduction Zone Segments

Summary statistics for surface heat flow observations by subduction zone segment are given in @tbl:hf.summary.table and @fig:hf.summary.plot.

```{r hf.summary.table, results = 'asis'}
# Summarize data
shp.hf.crop %>%
	st_set_geometry(NULL) %>%
	group_by(segment) %>%
	rename(hf = `heat-flow (mW/m2)`,
				 Segment = segment) %>%
	summarise(n = n(),
						Min = round(min(hf)),
						Max = round(max(hf)),
						Median =round(median(hf)),
						IQR = round(IQR(hf)),
						Mean = round(mean(hf)),
						Sigma = round(sd(hf))) %>%
	knitr::kable(caption = 'Summary of heat flow ($mWm^{-2}$) observations by subduction zone segment {#tbl:hf.summary.table}',
							 format = 'pandoc')
```

```{r hf.summary.plot, out.width='80%', fig.cap="Distribution of heat flow observations by subduction zone segment. Heat flow near most segments are centered around $50~mWm^{-2}$ and highly skewed right. The skeweness likely represents sampling near hydrothermal systems, volcanic arcs, or spreading centers."}
knitr::include_graphics('../figs/summary/hf_summary.png')
```

The optimal variogram models and associated errors $C_F(\Theta)$ and $C_I(\Theta)$ are given in @tbl:variogram.summary.table and @fig:variogram.summary.plot.

```{r variogram.summary.table}
purrr::map_df(d, ~.x$k$model.variogram, .id = 'segment') %>%
	as_tibble() %>%
	dplyr::select(segment, model, psill, range) %>%
	rename(Segment = segment, Model = model,
				 Sill = psill, Range = range) %>%
		knitr::kable(caption = 'Summary of optimal varigram models by subduction zone segment {#tbl:variogram.summary.table}',
								 format = 'pandoc')
```

```{r variogram.summary.plot, out.width='80%', fig.cap="optimal variogram models by subduction zone segment. the y-axis is semivariance $(mwm^{-2})^2$. about half of the experimental variograms (dots) deviate, often periodically, from the variogram models. deviations often occur between 500 to 1000 km."}
knitr::include_graphics('../figs/summary/variogram_summary.png')
```

Summary statistics for the interpolation differences are given in @tbl:diff.summary.table.

```{r diff.summary.table}
purrr::map_df(d,
							~.x$shp.hf.pred %>%
							st_set_geometry(NULL) %>%
							summarise(Min = round(min(HF_diff)),
												Max = round(max(HF_diff)),
												Median = round(median(HF_diff)),
												IQR = round(IQR(HF_diff)),
												Mean = round(mean(HF_diff)),
												Sigma = round(sd(HF_diff))),
							.id = 'Segment') %>%
		knitr::kable(caption = 'Summary of predicted heat flow differences by subduction zone segment {#tbl:diff.summary.table}',
								 format = 'pandoc')
```


# Discussion

# Conclusions

\acknowledgments

\clearpage

# References
